# Lab 5 - Apache Pig


## üóÇÔ∏è Structure du projet

```
Lab5_Pig/
‚îú‚îÄ‚îÄ pig_scripts/                  # Scripts Pig Latin
‚îÇ   ‚îú‚îÄ‚îÄ wordcount.pig             # Exemple WordCount (‚úÖ Termin√©)
‚îÇ   ‚îú‚îÄ‚îÄ employees.pig             # Analyse des employ√©s (‚úÖ Termin√©)
‚îÇ   ‚îú‚îÄ‚îÄ flights.pig               # Analyse des vols (‚úÖ Termin√©)
‚îÇ   ‚îú‚îÄ‚îÄ films.pig                 # Analyse des films (‚úÖ Termin√©)
‚îÇ   ‚îî‚îÄ‚îÄ convert_json_to_csv.py    # Script de conversion JSON‚ÜíCSV
‚îú‚îÄ‚îÄ data/                         # Donn√©es sources
‚îÇ   ‚îú‚îÄ‚îÄ alice.txt
‚îÇ   ‚îú‚îÄ‚îÄ employees.txt
‚îÇ   ‚îú‚îÄ‚îÄ departments.txt
‚îÇ   ‚îú‚îÄ‚îÄ films.json / films.csv
‚îÇ   ‚îú‚îÄ‚îÄ artists.json / artists.csv
‚îÇ   ‚îú‚îÄ‚îÄ film_actors.csv
‚îÇ   ‚îî‚îÄ‚îÄ flights/
‚îÇ       ‚îî‚îÄ‚îÄ sample_flights.csv
‚îú‚îÄ‚îÄ screenshots/                  # Captures d'√©cran des r√©sultats
‚îÇ   ‚îú‚îÄ‚îÄ 01_wordcount.png
‚îÇ   ‚îú‚îÄ‚îÄ 02_employees.png
‚îÇ   ‚îú‚îÄ‚îÄ 03_flights.png
‚îÇ   ‚îî‚îÄ‚îÄ 04_films.png
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ lab5_APACHE_PIG.pdf       # √ânonc√© du TP
‚îî‚îÄ‚îÄ README.md
```


## üîß Installation Apache Pig

Dans le conteneur `hadoop-master` :

```bash
# T√©l√©charger et installer Pig
wget https://dlcdn.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz
tar -zxvf pig-0.17.0.tar.gz
mv pig-0.17.0 /usr/local/pig
rm pig-0.17.0.tar.gz

# Configurer les variables d'environnement
echo 'export PIG_HOME=/usr/local/pig' >> ~/.bashrc
echo 'export PATH=$PATH:$PIG_HOME/bin' >> ~/.bashrc
source ~/.bashrc

# D√©marrer Hadoop et les services
./start-hadoop.sh
yarn timelineserver &
mapred --daemon start historyserver
```

## üìù Scripts r√©alis√©s

### 1. WordCount (`wordcount.pig`) ‚úÖ

Compte les occurrences de chaque mot dans le texte d'Alice au Pays des Merveilles.

**Ex√©cution :**
```bash
# Mode local
pig -x local
grunt> exec /shared_volume/wordcount.pig
```

**R√©sultats :**
- Fichier d'entr√©e : `alice.txt` (15 lignes)
- Sortie : `/shared_volume/pig_out/WORD_COUNT/`
- 119 mots uniques identifi√©s
- **Screenshot** : `screenshots/01_wordcount.png`

---

### 2. Analyse des employ√©s (`employees.pig`) ‚úÖ

Analyse compl√®te des donn√©es d'employ√©s d'une entreprise.

**Pr√©paration des donn√©es :**
```bash
# Copier les donn√©es sur HDFS
hdfs dfs -mkdir -p /input/employees
hdfs dfs -put /shared_volume/employees.txt /input/employees/
hdfs dfs -put /shared_volume/departments.txt /input/employees/
```

**Ex√©cution :**
```bash
pig /shared_volume/employees.pig
```

**Analyses effectu√©es :**
1. ‚úÖ Salaire moyen par d√©partement
2. ‚úÖ Nombre d'employ√©s par d√©partement
3. ‚úÖ Liste des employ√©s avec leurs d√©partements
4. ‚úÖ Employ√©s avec salaire > 60 000‚Ç¨
5. ‚úÖ D√©partement avec le salaire moyen le plus √©lev√©
6. ‚úÖ D√©partements sans employ√©s
7. ‚úÖ Nombre total d'employ√©s dans l'entreprise
8. ‚úÖ Employ√©s de la ville de Paris
9. ‚úÖ Salaire total par ville
10. ‚úÖ D√©partements ayant des femmes employ√©es (heuristique sur pr√©noms)

**R√©sultats :**
- 20 employ√©s analys√©s
- 6 d√©partements
- Sortie finale : `/pigout/employes_femmes/` (4 d√©partements)
- **Screenshot** : `screenshots/02_employees.png`

---

### 3. Analyse des vols a√©riens (`flights.pig`) ‚úÖ

Traitement et analyse de donn√©es de vols commerciaux.

**Pr√©paration :**
```bash
hdfs dfs -mkdir -p /input/flights
hdfs dfs -put /shared_volume/sample_flights.csv /input/flights/
```

**Analyses r√©alis√©es :**
- Top 20 a√©roports par volume total de vols (arriv√©es + d√©parts)
- Popularit√© des transporteurs (volume logarithmique par ann√©e)
- Proportion de vols retard√©s (retard > 15 min) par ann√©e
- Retards par transporteur et par ann√©e
- Itin√©raires les plus fr√©quent√©s

**R√©sultats :**
- Sortie : `/pigout/top_routes/`
- **Screenshot** : `screenshots/03_flights.png`

---

### 4. Analyse des films (`films.pig`) ‚úÖ

Traitement de donn√©es cin√©matographiques (films, r√©alisateurs, acteurs).

#### üìå Note importante sur le traitement JSON

L'√©nonc√© demandait de traiter directement les fichiers JSON avec `JsonLoader`. 
Cependant, nous avons rencontr√© les limitations suivantes :

- ‚ùå `JsonLoader` ne supporte pas les noms de champs commen√ßant par `_` (comme `_id`)
- ‚ùå Le format JSON pretty-printed n'est pas compatible avec `JsonLoader` (qui attend du JSON Lines)
- ‚ùå La biblioth√®que Piggybank pr√©sentait des bugs de compatibilit√©

**Solution adopt√©e** : Conversion JSON ‚Üí CSV via script Python (`convert_json_to_csv.py`), puis traitement avec `PigStorage`.

Cette approche est courante en production Big Data lorsque les donn√©es sources ne sont pas dans le format optimal pour l'outil de traitement.

**Pr√©paration :**
```bash
# 1. Convertir JSON en CSV (sur Windows)
python pig_scripts/convert_json_to_csv.py

# 2. Copier vers HDFS
hdfs dfs -mkdir -p /input/films
hdfs dfs -put /shared_volume/films.csv /input/films/
hdfs dfs -put /shared_volume/artists.csv /input/films/
hdfs dfs -put /shared_volume/film_actors.csv /input/films/
```

**Analyses effectu√©es :**
1. Films am√©ricains group√©s par ann√©e
2. Films am√©ricains group√©s par r√©alisateur
3. Extraction des acteurs (triplets film-acteur-r√¥le)
4. Jointure films + acteurs avec informations compl√®tes
5. Films complets avec tous leurs acteurs (COGROUP)
6. Acteurs/R√©alisateurs : nombre de films jou√©s ET r√©alis√©s par artiste

## üì∏ Captures d'√©cran

Toutes les captures d'√©cran des r√©sultats sont disponibles dans le dossier `screenshots/`.

